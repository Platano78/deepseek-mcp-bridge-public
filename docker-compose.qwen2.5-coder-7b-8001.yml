version: '3.8'

services:
  qwen25-coder-7b-8001:
    image: vllm/vllm-openai:latest
    container_name: qwen25-coder-7b-8001
    ports:
      - "8001:8000"  # Map host port 8001 to container port 8000
      - "3018:3018"  # Monitor port (different from existing)
    environment:
      # Model configuration
      - MODEL_NAME=wordslab-org/Qwen2.5-Coder-7B-Instruct-FP8-Dynamic
      - SERVED_MODEL_NAME=qwen2.5-coder-7b-fp8-dynamic

      # YARN Configuration for 131k context
      - MAX_MODEL_LEN=131072
      - ROPE_SCALING_TYPE=yarn
      - ROPE_SCALING_FACTOR=4.0
      - ROPE_THETA=500000

      # Performance and GPU settings optimized for RTX 5080 16GB
      - GPU_MEMORY_UTILIZATION=0.90
      - TENSOR_PARALLEL_SIZE=1
      - DTYPE=auto

      # API configuration
      - HOST=0.0.0.0
      - PORT=8000
      - WORKER_USE_RAY=false
      - ENGINE_USE_RAY=false

      # Cache and performance optimizations
      - ENABLE_PREFIX_CACHING=true
      - MAX_PADDINGS=256

      # Health monitoring
      - HEALTH_CHECK_GRACE_PERIOD=30

    volumes:
      # Model cache volume for persistence
      - model_cache_8001:/root/.cache/huggingface
      # Mount local model directory for caching
      - ./model_cache:/root/.cache/huggingface:rw

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    command: >
      --model wordslab-org/Qwen2.5-Coder-7B-Instruct-FP8-Dynamic
      --served-model-name qwen2.5-coder-7b-fp8-dynamic
      --host 0.0.0.0
      --port 8000
      --max-model-len 131072
      --gpu-memory-utilization 0.90
      --dtype auto
      --tensor-parallel-size 1
      --enable-prefix-caching
      --rope-scaling '{"type": "yarn", "factor": 4.0, "original_max_position_embeddings": 32768}'
      --rope-theta 500000

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

    restart: unless-stopped

    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

    labels:
      - "traefik.enable=false"
      - "service.name=qwen25-coder-7b-8001"
      - "service.version=1.0.0"
      - "service.description=Qwen2.5-Coder-7B with YARN 131k context on port 8001"

volumes:
  model_cache_8001:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/model_cache

networks:
  default:
    name: vllm-network
    external: false